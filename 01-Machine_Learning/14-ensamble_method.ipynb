{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Methods\n",
    "Ensamble methods is a machine learning technique that combine several base models  in order to produce one optimal predictive model.\n",
    "## Types of Ensamble Methods\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "2. Boosting\n",
    "3. stacking(Stacked Generalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating)\n",
    "1. Bagging reduce variance and helps to avoid overfiting\n",
    "2. It involves creating multiple subsets of original datasets with replacements (bootstraping)\n",
    "3. Traning a model on each subset\n",
    "4. And then aggerating their predications\n",
    "5. A common example is Random forest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "1. Boosting turning weak learners into strong ones iteratively\n",
    "2. It adjust weight of an observation based on the last classification \n",
    "3. If the observation is classifiec incorrectly it tries to increase the weight of this observation vice versa\n",
    "4. Examples include **`AdaBoost`** **Gradient Boost** and **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking (Stacked Generilzation )\n",
    "1. Stacking means training a new model in order to combine the predicitions of several base model.\n",
    "2. It typically involves two base level models and a meat levl model that make a final prediction based on base level models outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why to use Ensamble Methods\n",
    "**`Accuracy`**  Ensamble methods provide often higher accuracy by single models by combining their strengths and compensation for their weaknesses.\\\n",
    "**`stability`** They are less prone to error due to variane in the data,making them more reliable\\\n",
    "**`Reduced Overfiting`** Techniques like bagging and boosting help reducing overfiting making them more generalizable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Ensamble Methods:\n",
    "Ensamble methods are used across wide range of fields \\\n",
    "In **`finance`** for credit scoring and algorithmic trading.\\\n",
    "In **`Health care`** for dieases prediction and diagnosis.\\\n",
    "In **`e-comerece`** is used in recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges and Considerations\n",
    "**`Complexity`** Ensamble models can be more complex and compuntationally more expensive then single models.\\\n",
    "**`Interpretability`** increased complexity can leads to models that are harder to interpret\\\n",
    "**`Parameter tuning`** Requires carefull tuning of paameters and appropiate selection of base leaners\\\n",
    "Ensamble algorithms are powerfull too in data scientist arsenal, offering enhanced accuracy,roubtness,and generlizability.By understanding and untilizing these techniques, one can significantly improve the performance of the machine learning models in various field of real World applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest is powerfull machine learning alogrithm that belong to the ensamble learning family. it's know for simplicity and ability to run on large datasets. \\\n",
    "Random forest is ensamble learning technique that is used for both classification and Regression task.\\\n",
    "It operates by constructing a multitude of decision trees at the training time and outputing the class that is mode of the classes (in the case of classification) or mean predication (in the case regression) of the individual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Random forest work?\n",
    "Random forest creates mutlitple Decision trees using bagging.It randomnly selects samples from the dataset with replacments and build a decision tree for each sample.\n",
    "## Random feature selection \n",
    "in each split of the tree construction, a random subset of features is considered. This randomness helps model roubtness and prevent overfiting.\n",
    "## Aggregating the results \n",
    "For classification most voted class become the model precition.For Regression It, averages the output of different trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features Of Random Forest\n",
    "#### Robustness \n",
    "it's less prone to overfiting because it combine roubtness of many decision trees\n",
    "#### Handling non linearity\n",
    "Capable of handling non linear data with it's multiple tree structure \n",
    "#### Feature Importance\n",
    "it provides insingts into the feature which are more important for predication.\n",
    "#### High Accuracy \n",
    "it often provides high accuracy then other models \n",
    "#### Versatility\n",
    "Can be used for both classification and regresssion tasks.\n",
    "#### Ease of use \n",
    "Require litle bit of tuning and easy to use for the beginners.\n",
    "### Applications of Random Forest\n",
    "1. Banking for credit scoring \n",
    "2. E-commerce for recommending products.\n",
    "3. In health care for diases prediction and diagonsis.\n",
    "4. For stock market predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices and consideration\n",
    "#### 1. Number of trees\n",
    "More trees can increase the accuracy but also computational cost.\n",
    "#### 2. Depth of trees.\n",
    "Deeper trees can catch more information but might lead to overfiting.\n",
    "#### 3. Feature selection \n",
    "it is important to ensure that features are revalent and good quality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.54      0.44        13\n",
      "           1       0.80      0.67      0.73        36\n",
      "\n",
      "    accuracy                           0.63        49\n",
      "   macro avg       0.58      0.60      0.58        49\n",
      "weighted avg       0.69      0.63      0.65        49\n",
      "\n",
      "confusion matrix:\n",
      " [[ 7  6]\n",
      " [12 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing libraries \n",
    "import pandas as pf \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px \n",
    "# Machine learning libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# load the dataset\n",
    "df= px.data.tips()\n",
    "# preprocess the data \n",
    "le=LabelEncoder()\n",
    "# constructing a for loop\n",
    "for col in df.columns:\n",
    "    if df[col].dtype=='object':\n",
    "        df[col]=le.fit_transform(df[col])\n",
    "# split the data into x and y \n",
    "x=df.drop('sex',axis=1)\n",
    "y=df['sex']\n",
    "# now let's call model here \n",
    "model=RandomForestClassifier()\n",
    "#now split the data into training and testing data \n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# now fit the model \n",
    "model.fit(x_train,y_train)\n",
    "# now predication from the model\n",
    "y_pred=model.predict(x_test)\n",
    "# Evaluate the model\n",
    "print(\"classification_report\\n\",classification_report(y_pred,y_test))\n",
    "print(\"confusion matrix:\\n\",confusion_matrix(y_pred,y_test))\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test),annot=True ,cmap=\"Blues\",cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9853549934693892\n",
      "RMSE: 0.9926504890793079\n",
      "r2_score: 0.30097031584825384\n",
      "MAE: 0.7907673469387758\n"
     ]
    }
   ],
   "source": [
    "# let's call the model here \n",
    "model=RandomForestRegressor()\n",
    "x=df.drop('tip',axis=1)\n",
    "y=df['tip']\n",
    "# training and testing data \n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# now fit the model \n",
    "model.fit(x_train,y_train)\n",
    "# now predict the model\n",
    "y_pred=model.predict(x_test)\n",
    "# evaluate the model \n",
    "print('MSE:',mean_squared_error(y_pred,y_test))\n",
    "print('RMSE:', (np.sqrt(mean_squared_error(y_pred,y_test))))\n",
    "print('r2_score:', r2_score(y_pred,y_test))\n",
    "print('MAE:', mean_absolute_error(y_pred,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
