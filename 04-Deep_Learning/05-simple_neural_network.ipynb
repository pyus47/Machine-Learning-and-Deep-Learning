{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps before creating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "# import warnings \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "# load the titanic dataset \n",
    "df = sns.load_dataset('titanic')\n",
    "# preprocessing of titanic data \n",
    "# removing the rows with missing values \n",
    "df.dropna(subset=['age','embarked','embark_town'],inplace=True)\n",
    "# encoding the categorical variables to dummy variables \n",
    "df = pd.get_dummies(df,columns=['sex','embarked','class','who','alive'])\n",
    "# selecting features  and target \n",
    "x = df.drop(['embark_town','alone','deck','survived','adult_male'],axis=1)\n",
    "y = df['survived']\n",
    "# spliting the datasets \n",
    "x_train, x_test, y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# standarize the data \n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.5446 - accuracy: 0.6573\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4595 - accuracy: 0.8067\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3848 - accuracy: 0.9174\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3223 - accuracy: 0.9279\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.2760 - accuracy: 0.9402\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.2404 - accuracy: 0.9561\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.2120 - accuracy: 0.9631\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.9649\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9789\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9859\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.1198 - accuracy: 0.9965\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9965\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0973 - accuracy: 0.9982\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9982\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9982\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9982\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 0.9982\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0600 - accuracy: 0.9982\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9982\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9930\n",
      "Test Accuracy: 0.9930070042610168\n",
      "Test loss: 0.07257115095853806\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "input_layer = tf.keras.layers.Dense(10, activation= 'relu',input_shape=(x_train.shape[1],)) # input layer                                  \n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid') # output_layer\n",
    "# building the model \n",
    "model = tf.keras.models.Sequential([input_layer,output_layer])\n",
    "# complie the model \n",
    "model.compile(optimizer = 'adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# train the model using fit function \n",
    "model.fit(x_train,y_train,epochs=20,batch_size=32,verbose=1)\n",
    "# Evaluate the model \n",
    "loss, accuracy = model.evaluate(x_test,y_test,verbose=1)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Test loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
